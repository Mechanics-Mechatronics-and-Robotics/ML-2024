{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import time\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 100\n",
    "random_seeds = [42, 0, 17, 9, 3, 16, 2]  # model seeds are used for deep ensembling\n",
    "lr = 0.001  # learning rate\n",
    "WD = 0.001  # weight decay (L2 regularization)\n",
    "DO = 0.1  # dropout (at the training stage only)\n",
    "\n",
    "# Transformer model\n",
    "def create_transformer_model(input_shape, num_heads=4, ff_dim=64, num_transformer_blocks=2, do=DO):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        # Multi-Head Attention\n",
    "        attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=input_shape[-1])(x, x)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x + attention_output)\n",
    "\n",
    "        # Feed-forward network\n",
    "        ffn = tf.keras.Sequential(\n",
    "            [Dense(ff_dim, activation=sinOcos), Dense(input_shape[-1])]\n",
    "        )\n",
    "        ffn_output = ffn(x)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x + ffn_output)\n",
    "\n",
    "    # Output layer\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dropout(do)(x)\n",
    "    outputs = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Learning rate scheduler\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.96, patience=10, min_lr=1e-7, verbose=1)\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=num_epochs, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Split the data\n",
    "X_train_s, X_val, y_train_s, y_val = train_test_split(X_train, y_train, test_size=0.1)\n",
    "\n",
    "# Reshape input data for Transformer\n",
    "# Transformer expects 3D input: (batch_size, sequence_length, features)\n",
    "X_train_reshaped = X_train_s.reshape(X_train_s.shape[0], 1, X_train_s.shape[1])\n",
    "X_val_reshaped = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "# Train multiple models with different random seeds\n",
    "models = []\n",
    "start = time.time()\n",
    "for seed in random_seeds:\n",
    "    tf.random.set_seed(seed)\n",
    "    model = create_transformer_model(input_shape=(1, X_train_s.shape[1]))\n",
    "    model.compile(loss=\"mse\", optimizer=Adam(learning_rate=lr, decay=WD))\n",
    "    history = model.fit(\n",
    "        x=X_train_reshaped,\n",
    "        y=y_train_s,\n",
    "        validation_data=(X_val_reshaped, y_val),\n",
    "        epochs=num_epochs,\n",
    "        batch_size=500,\n",
    "        callbacks=[reduce_lr, early_stop]\n",
    "    )\n",
    "    models.append(model)\n",
    "end_train = time.time()\n",
    "\n",
    "# Aggregate predictions from the models\n",
    "y_predictions = np.zeros((X_test_reshaped.shape[0], len(models)))\n",
    "for i, model in enumerate(models):\n",
    "    y_predictions[:, i] = model.predict(X_test_reshaped).flatten()\n",
    "\n",
    "# Compute the ensemble prediction\n",
    "ensemble_prediction = np.mean(y_predictions, axis=1)\n",
    "end_predict = time.time()\n",
    "\n",
    "# Model performance\n",
    "model_performance.loc['Transformer+MSE'] = [\n",
    "    sklearn.metrics.r2_score(y_test, ensemble_prediction),\n",
    "    mean_squared_error(y_test, ensemble_prediction, squared=False),\n",
    "    end_train - start,\n",
    "    end_predict - end_train,\n",
    "    end_predict - start\n",
    "]\n",
    "\n",
    "print('R-squared error: ' + \"{:.2%}\".format(sklearn.metrics.r2_score(y_test, ensemble_prediction)))\n",
    "print('Root Mean Squared Error: ' + \"{:.2f}\".format(mean_squared_error(y_test, ensemble_prediction, squared=False)))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
